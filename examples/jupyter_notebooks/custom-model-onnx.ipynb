{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca94ca71",
   "metadata": {},
   "source": [
    "# Custom Model Compilation and Inference using Onnx runtime \n",
    "\n",
    "In this example notebook, we describe how to take a pre-trained classification model and compile it using ***Onnx runtime*** to generate deployable artifacts that can be deployed on the target using the ***Onnx*** interface. \n",
    " \n",
    " - Pre-trained model: `resnet18v2` model trained on ***ImageNet*** dataset using ***Onnx***  \n",
    " \n",
    "In particular, we will show how to\n",
    "- compile the model (during heterogenous model compilation, layers that are supported will be offloaded to the`TI-DSP` and artifacts needed for inference are generated)\n",
    "- use the generated artifacts for inference\n",
    "- perform input preprocessing and output postprocessing\n",
    "    \n",
    "## Onnx Runtime based work flow\n",
    "\n",
    "The diagram below describes the steps for Onnx Runtime based work flow. \n",
    "\n",
    "Note:\n",
    " - The user needs to compile models(sub-graph creation and quantization) on a PC to generate model artifacts.\n",
    " - The generated artifacts can then be used to run inference on the target.\n",
    "\n",
    "<img src=docs/images/onnx_work_flow_2.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19673e60",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from scripts.utils import imagenet_class_to_name, download_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dc078",
   "metadata": {},
   "source": [
    "## Define utility function to preprocess input images\n",
    "Below, we define a utility function to preprocess images for `resnet18v2`. This function takes a path as input, loads the image and preprocesses it for generic ***Onnx*** inference. The steps are as follows: \n",
    "\n",
    " 1. load image\n",
    " 2. convert BGR image to RGB\n",
    " 3. scale image so that the short edge is 256 pixels\n",
    " 4. center-crop image to 224x224 pixels\n",
    " 5. apply per-channel pixel scaling and mean subtraction\n",
    "\n",
    "\n",
    "- Note: If you are using a custom model or a model that was trained using a different framework, please remember to define your own utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_onnx_resent18v2(image_path):\n",
    "    \n",
    "    # read the image using openCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert to RGB\n",
    "    img = img[:,:,::-1]\n",
    "    \n",
    "    # Most of the onnx models are trained using\n",
    "    # 224x224 images. The general rule of thumb\n",
    "    # is to scale the input image while preserving\n",
    "    # the original aspect ratio so that the\n",
    "    # short edge is 256 pixels, and then\n",
    "    # center-crop the scaled image to 224x224\n",
    "    orig_height, orig_width, _ = img.shape\n",
    "    short_edge = min(img.shape[:2])\n",
    "    new_height = (orig_height * 256) // short_edge\n",
    "    new_width = (orig_width * 256) // short_edge\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    startx = new_width//2 - (224//2)\n",
    "    starty = new_height//2 - (224//2)\n",
    "    img = img[starty:starty+224,startx:startx+224]\n",
    "    \n",
    "    # apply scaling and mean subtraction.\n",
    "    # if your model is built with an input\n",
    "    # normalization layer, then you might\n",
    "    # need to skip this\n",
    "    img = img.astype('float32')\n",
    "    for mean, scale, ch in zip([128, 128, 128], [0.0078125, 0.0078125, 0.0078125], range(img.shape[2])):\n",
    "            img[:,:,ch] = ((img.astype('float32')[:,:,ch] - mean) * scale)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = np.transpose(img, (0, 3, 1, 2))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db7a1b",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "In this step, we create Onnx runtime with `tidl_model_import_onnx` library to generate artifacts that offload supported portion of the DL model to the TI DSP.\n",
    " - `sess` is created with the options below to calibrate the model for 8-bit fixed point inference\n",
    "   \n",
    "    * **artifacts_folder** - folder where all the compilation artifacts needed for inference are stored \n",
    "    * **tidl_tools_path** - os.getenv('TIDL_TOOLS_PATH'), path to `TIDL` compilation tools \n",
    "    * **tensor_bits** - 8 or 16, is the number of bits to be used for quantization \n",
    "    * **advanced_options:calibration_frames**  - number of images to be used for calibration\n",
    "     \n",
    "    ``` \n",
    "    compile_options = {\n",
    "        'tidl_tools_path' : os.environ['TIDL_TOOLS_PATH'],\n",
    "        'artifacts_folder' : output_dir,\n",
    "        'tensor_bits' : 16,\n",
    "        'accuracy_level' : 0,\n",
    "        'advanced_options:calibration_frames' : len(calib_images), \n",
    "        'advanced_options:calibration_iterations' : 3 # used if accuracy_level = 1\n",
    "    }\n",
    "    ``` \n",
    "    \n",
    "- Note: The path to `TIDL` compilation tools and `aarch64` `GCC` compiler is required for model compilation, both of which are accessed by this notebook using predefined environment variables `TIDL_TOOLS_PATH` and `ARM64_GCC_PATH`. The example usage of both the variables is demonstrated in the cell below. \n",
    "- `accuracy_level` is set to 0 in this example. For better accuracy, set `accuracy_level = 1`. This option results in more time for compilation but better inference accuracy. \n",
    "Compilation status log for accuracy_level = 1 is currently not implemented in this notebook. This will be added in future versions. \n",
    "- Please refer to TIDL user guide for further advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81200ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'custom-artifacts/onnx/resnet18v2'\n",
    "onnx_model_path = '../../models/public/onnx/resnet18_opset9.onnx'\n",
    "download_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc50b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_images = [\n",
    "'sample-images/elephant.bmp',\n",
    "'sample-images/bus.bmp',\n",
    "'sample-images/bicycle.bmp',\n",
    "'sample-images/zebra.bmp',\n",
    "]\n",
    "\n",
    "compile_options = {\n",
    "    'tidl_tools_path' : os.environ['TIDL_TOOLS_PATH'],\n",
    "    'artifacts_folder' : output_dir,\n",
    "    'tensor_bits' : 8,\n",
    "    'accuracy_level' : 1,\n",
    "    'advanced_options:calibration_frames' : len(calib_images), \n",
    "    'advanced_options:calibration_iterations' : 3 # used if accuracy_level = 1\n",
    "}\n",
    "\n",
    "# create the output dir if not present\n",
    "# clear the directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for root, dirs, files in os.walk(output_dir, topdown=False):\n",
    "    [os.remove(os.path.join(root, f)) for f in files]\n",
    "    [os.rmdir(os.path.join(root, d)) for d in dirs]\n",
    "\n",
    "so = rt.SessionOptions()\n",
    "EP_list = ['TIDLCompilationProvider','CPUExecutionProvider']\n",
    "sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[compile_options, {}], sess_options=so)\n",
    "\n",
    "input_details = sess.get_inputs()\n",
    "\n",
    "for num in tqdm.trange(len(calib_images)):\n",
    "    output = list(sess.run(None, {input_details[0].name : preprocess_for_onnx_resent18v2(calib_images[num])}))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9c1ea",
   "metadata": {},
   "source": [
    "## Use compiled model for inference\n",
    "Then using ***Onnx*** with the ***`libtidl_onnxrt_EP`*** inference library we run the model and collect benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_list = ['TIDLExecutionProvider','CPUExecutionProvider']\n",
    "\n",
    "sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[compile_options, {}], sess_options=so)\n",
    "#Running inference several times to get an stable performance output\n",
    "for i in range(5):\n",
    "    output = list(sess.run(None, {input_details[0].name : preprocess_for_onnx_resent18v2('sample-images/elephant.bmp')}))\n",
    "\n",
    "for idx, cls in enumerate(output[0].squeeze().argsort()[-5:][::-1]):\n",
    "    print('[%d] %s' % (idx, '/'.join(imagenet_class_to_name(cls))))\n",
    "    \n",
    "from scripts.utils import plot_TI_performance_data, plot_TI_DDRBW_data, get_benchmark_output\n",
    "stats = sess.get_TI_benchmark_data()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "plot_TI_performance_data(stats, axis=ax)\n",
    "plt.show()\n",
    "\n",
    "tt, st, rb, wb = get_benchmark_output(stats)\n",
    "print(f'Statistics : \\n Inferences Per Second   : {1000.0/tt :7.2f} fps')\n",
    "print(f' Inferece Time Per Image : {tt :7.2f} ms  \\n DDR BW Per Image        : {rb+ wb : 7.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
